{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python382jvsc74a57bd038740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b",
   "display_name": "Python 3.8.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper parameters\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "167684\n167684\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"cleaned.csv\", 'r',encoding='utf8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)\n",
    "print(len(labels))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    \n",
    "    # Removing html tags\n",
    "    sentence = str(remove_tags(sen))\n",
    "    # Remove punctuations and numbers\n",
    "\n",
    "    sentence = str(re.sub('[^a-zA-Z]', ' ', sentence))\n",
    "    \n",
    "\n",
    "    # Single character removal\n",
    "    sentence = str(re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence))\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = str(re.sub(r'\\s+', ' ', sentence))\n",
    "\n",
    "    sentence = str(sentence.lower())\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>') ##\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_articles = []\n",
    "for sen in articles:\n",
    "    X_articles.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = []\n",
    "for sen in labels:\n",
    "    Y_labels.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'great service amazing support '"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_articles[1192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\n",
    "Y_labels[11192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "134147\n134147\n134147\n33537\n33537\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(X_articles) * training_portion)\n",
    "\n",
    "train_articles = X_articles[0: train_size]\n",
    "train_labels = Y_labels[0: train_size]\n",
    "\n",
    "validation_articles = X_articles[train_size:]\n",
    "validation_labels = Y_labels[train_size:]\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'i': 2,\n",
       " 'service': 3,\n",
       " 'call': 4,\n",
       " 'account': 5,\n",
       " 'the': 6,\n",
       " 'get': 7,\n",
       " 'told': 8,\n",
       " 'back': 9,\n",
       " 'dstv': 10}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[22, 126, 1, 721, 156, 694, 116, 1410, 246, 3062]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_articles)\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n200\n114\n200\n10\n200\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(len(train_sequences[0]))\n",
    "print(len(train_padded[0]))\n",
    "\n",
    "print(len(train_sequences[1]))\n",
    "print(len(train_padded[1]))\n",
    "\n",
    "print(len(train_sequences[10]))\n",
    "print(len(train_padded[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33537\n(33537, 200)\n"
     ]
    }
   ],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3]\n[2]\n[3]\n(134147, 1)\n[4]\n[4]\n[2]\n(33537, 1)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
    "print(training_label_seq[0])\n",
    "print(training_label_seq[1])\n",
    "print(training_label_seq[2])\n",
    "print(training_label_seq.shape)\n",
    "\n",
    "print(validation_label_seq[0])\n",
    "print(validation_label_seq[1])\n",
    "print(validation_label_seq[2])\n",
    "print(validation_label_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, None, 64)          320000    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 128)               98816     \n_________________________________________________________________\ndense_3 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_4 (Dense)              (None, 6)                 390       \n=================================================================\nTotal params: 427,462\nTrainable params: 427,462\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of size 5000, and output embedding dimension of size 64 we set at the top\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    # use ReLU in place of tanh function since they are very good alternatives of each other.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 6 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'neutral', 'worst', 'great', 'good', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1049/1049 - 479s - loss: 0.2625 - accuracy: 0.9047 - val_loss: 2.6471 - val_accuracy: 0.3460\n",
      "Epoch 2/10\n",
      "1049/1049 - 417s - loss: 0.2352 - accuracy: 0.9169 - val_loss: 2.8126 - val_accuracy: 0.3518\n",
      "Epoch 3/10\n",
      "1049/1049 - 451s - loss: 0.2170 - accuracy: 0.9254 - val_loss: 2.8001 - val_accuracy: 0.3719\n",
      "Epoch 4/10\n",
      "1049/1049 - 401s - loss: 0.2001 - accuracy: 0.9328 - val_loss: 3.2743 - val_accuracy: 0.3367\n",
      "Epoch 5/10\n",
      "1049/1049 - 488s - loss: 0.1863 - accuracy: 0.9386 - val_loss: 3.1098 - val_accuracy: 0.3689\n",
      "Epoch 6/10\n",
      "1049/1049 - 542s - loss: 0.1747 - accuracy: 0.9439 - val_loss: 3.2709 - val_accuracy: 0.3456\n",
      "Epoch 7/10\n",
      "1049/1049 - 396s - loss: 0.1646 - accuracy: 0.9480 - val_loss: 3.2616 - val_accuracy: 0.3763\n",
      "Epoch 8/10\n",
      "1049/1049 - 418s - loss: 0.1566 - accuracy: 0.9511 - val_loss: 3.3759 - val_accuracy: 0.3725\n",
      "Epoch 9/10\n",
      "1049/1049 - 462s - loss: 0.1467 - accuracy: 0.9545 - val_loss: 3.6449 - val_accuracy: 0.3586\n",
      "Epoch 10/10\n",
      "1049/1049 - 552s - loss: 0.1419 - accuracy: 0.9562 - val_loss: 3.4861 - val_accuracy: 0.3680\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "history = model.fit(train_padded, training_label_seq, batch_size=128, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5.2056485e-08 6.6332418e-01 1.9508837e-01 8.4212855e-02 1.2742748e-02\n  4.4631843e-02]]\n1\nneutral\n"
     ]
    }
   ],
   "source": [
    "txt = [\"\"]\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(txt)\n",
    "padded = pad_sequences(seq,max_length)\n",
    "pred = model.predict(padded)\n",
    "rating = ['neutral', 'worst', 'great', 'good', 'bad']\n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))\n",
    "print(rating[np.argmax(pred)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}